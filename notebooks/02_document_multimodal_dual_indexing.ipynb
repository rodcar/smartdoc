{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (11.2.1)\n",
      "Requirement already satisfied: chromadb in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (1.0.12)\n",
      "Requirement already satisfied: tqdm in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: psutil in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (7.0.0)\n",
      "Requirement already satisfied: numpy in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: pytesseract in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (2.11.5)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (4.14.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.55b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.34.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (1.72.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (1.3.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.55b0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.55b0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
      "Requirement already satisfied: filelock in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Installing packages for multimodal indexing (image + OCR text)...\n",
      "ğŸ¨ OpenCLIP for image embeddings + ğŸ“ Tesseract for OCR text extraction\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for multimodal image + OCR text embedding\n",
    "%pip install pillow chromadb tqdm psutil numpy pytesseract\n",
    "\n",
    "# Note: ChromaDB's OpenCLIPEmbeddingFunction will automatically handle OpenCLIP dependencies\n",
    "print(\"ğŸ“¦ Installing packages for multimodal indexing (image + OCR text)...\")\n",
    "print(\"ğŸ¨ OpenCLIP for image embeddings + ğŸ“ Tesseract for OCR text extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports loaded successfully!\n",
      "ğŸ’» System: 14 CPU cores, 48.0GB RAM\n",
      "ğŸ¨ OpenCLIP + ğŸ“ OCR integration ready for multimodal indexing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions.open_clip_embedding_function import OpenCLIPEmbeddingFunction\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count\n",
    "import psutil\n",
    "import pytesseract\n",
    "\n",
    "# Fix tokenizers parallelism warning when using with pytesseract\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"âœ… Imports loaded successfully!\")\n",
    "print(f\"ğŸ’» System: {cpu_count()} CPU cores, {psutil.virtual_memory().total // (1024**3):.1f}GB RAM\")\n",
    "print(\"ğŸ¨ OpenCLIP + ğŸ“ OCR integration ready for multimodal indexing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents path: ../output/docs-sm_samples\n",
      "ChromaDB path: ../chroma_db_dual\n",
      "Image collection: smartdoc_images\n",
      "Text collection: smartdoc_texts\n",
      "Supported formats: .bmp, .jpg, .tiff, .jpeg, .png\n",
      "ğŸ¯ Mode: DUAL indexing (separate OpenCLIP images + SentenceTransformer text)\n"
     ]
    }
   ],
   "source": [
    "# Configuration for DUAL indexing (separate image and text collections)\n",
    "DOCS_PATH = \"../output/docs-sm_samples\"  # Fixed path relative to notebooks folder\n",
    "CHROMA_DB_PATH = \"../chroma_db_dual\"  # Store ChromaDB for dual embeddings\n",
    "IMAGE_COLLECTION_NAME = \"smartdoc_images\"\n",
    "TEXT_COLLECTION_NAME = \"smartdoc_texts\"\n",
    "\n",
    "# Supported image extensions\n",
    "IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.tiff', '.bmp'}\n",
    "\n",
    "print(f\"Documents path: {DOCS_PATH}\")\n",
    "print(f\"ChromaDB path: {CHROMA_DB_PATH}\")\n",
    "print(f\"Image collection: {IMAGE_COLLECTION_NAME}\")\n",
    "print(f\"Text collection: {TEXT_COLLECTION_NAME}\")\n",
    "print(f\"Supported formats: {', '.join(IMAGE_EXTENSIONS)}\")\n",
    "print(\"ğŸ¯ Mode: DUAL indexing (separate OpenCLIP images + SentenceTransformer text)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder: form\n",
      "Scanning folder: news_article\n",
      "Scanning folder: handwritten\n",
      "Scanning folder: resume\n",
      "Scanning folder: letter\n",
      "Scanning folder: specification\n",
      "Scanning folder: questionnaire\n",
      "Scanning folder: memo\n",
      "Scanning folder: scientific_report\n",
      "Scanning folder: scientific_publication\n",
      "Scanning folder: file_folder\n",
      "Scanning folder: advertisement\n",
      "Scanning folder: presentation\n",
      "Scanning folder: email\n",
      "Scanning folder: invoice\n",
      "Scanning folder: budget\n",
      "Found 3494 image files total\n",
      "\n",
      "Document type distribution:\n",
      "  advertisement: 229 files\n",
      "  budget: 247 files\n",
      "  email: 203 files\n",
      "  file_folder: 218 files\n",
      "  form: 229 files\n",
      "  handwritten: 226 files\n",
      "  invoice: 211 files\n",
      "  letter: 222 files\n",
      "  memo: 219 files\n",
      "  news_article: 187 files\n",
      "  presentation: 223 files\n",
      "  questionnaire: 217 files\n",
      "  resume: 217 files\n",
      "  scientific_publication: 219 files\n",
      "  scientific_report: 217 files\n",
      "  specification: 210 files\n"
     ]
    }
   ],
   "source": [
    "def get_all_image_files(base_path):\n",
    "    \"\"\"\n",
    "    Recursively find all image files in the document folders.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Base path to search for image files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (file_path, document_type)\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"Warning: Path {base_path} does not exist!\")\n",
    "        return image_files\n",
    "    \n",
    "    # Iterate through document type folders\n",
    "    for doc_type_folder in base_path.iterdir():\n",
    "        if doc_type_folder.is_dir():\n",
    "            doc_type = doc_type_folder.name\n",
    "            print(f\"Scanning folder: {doc_type}\")\n",
    "            \n",
    "            # Find all image files in this folder\n",
    "            for file_path in doc_type_folder.iterdir():\n",
    "                if file_path.is_file() and file_path.suffix.lower() in IMAGE_EXTENSIONS:\n",
    "                    image_files.append((str(file_path), doc_type))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files total\")\n",
    "    return image_files\n",
    "\n",
    "# Get all image files\n",
    "image_files = get_all_image_files(DOCS_PATH)\n",
    "\n",
    "# Show some statistics\n",
    "doc_types = {}\n",
    "for _, doc_type in image_files:\n",
    "    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "\n",
    "print(\"\\nDocument type distribution:\")\n",
    "for doc_type, count in sorted(doc_types.items()):\n",
    "    print(f\"  {doc_type}: {count} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ Setting up ChromaDB with DUAL indexing approach...\n",
      "ğŸ“ Creating separate collections for IMAGE and TEXT embeddings!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenCLIP embedding function initialized for image content\n",
      "âœ… SentenceTransformer embedding function initialized for text content\n",
      "âœ… Created new IMAGE collection 'smartdoc_images'\n",
      "âœ… Created new TEXT collection 'smartdoc_texts'\n",
      "ğŸ“ ChromaDB storage location: /Users/ivan/Workspace/agentai-document-data-extractor/smartdoc/chroma_db_dual\n",
      "ğŸ¯ Dual indexing collections ready for separate image and text embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB with DUAL indexing approach\n",
    "print(\"ğŸ¨ Setting up ChromaDB with DUAL indexing approach...\")\n",
    "print(\"ğŸ“ Creating separate collections for IMAGE and TEXT embeddings!\")\n",
    "\n",
    "# Import required embedding functions\n",
    "from chromadb.utils.embedding_functions.sentence_transformer_embedding_function import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Create separate embedding functions\n",
    "image_ef = OpenCLIPEmbeddingFunction()\n",
    "text_ef = SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"âœ… OpenCLIP embedding function initialized for image content\")\n",
    "print(\"âœ… SentenceTransformer embedding function initialized for text content\")\n",
    "\n",
    "# Create ChromaDB client with persistent storage\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "# Create or get IMAGE collection\n",
    "try:\n",
    "    image_collection = client.get_collection(name=IMAGE_COLLECTION_NAME)\n",
    "    print(f\"âœ… Loaded existing IMAGE collection '{IMAGE_COLLECTION_NAME}' with {image_collection.count()} documents\")\n",
    "except:\n",
    "    image_collection = client.create_collection(\n",
    "        name=IMAGE_COLLECTION_NAME,\n",
    "        embedding_function=image_ef,\n",
    "        metadata={\"description\": \"SmartDoc image embeddings using OpenCLIP\"}\n",
    "    )\n",
    "    print(f\"âœ… Created new IMAGE collection '{IMAGE_COLLECTION_NAME}'\")\n",
    "\n",
    "# Create or get TEXT collection  \n",
    "try:\n",
    "    text_collection = client.get_collection(name=TEXT_COLLECTION_NAME)\n",
    "    print(f\"âœ… Loaded existing TEXT collection '{TEXT_COLLECTION_NAME}' with {text_collection.count()} documents\")\n",
    "except:\n",
    "    text_collection = client.create_collection(\n",
    "        name=TEXT_COLLECTION_NAME,\n",
    "        embedding_function=text_ef,\n",
    "        metadata={\"description\": \"SmartDoc text embeddings using SentenceTransformer\"}\n",
    "    )\n",
    "    print(f\"âœ… Created new TEXT collection '{TEXT_COLLECTION_NAME}'\")\n",
    "\n",
    "print(f\"ğŸ“ ChromaDB storage location: {os.path.abspath(CHROMA_DB_PATH)}\")\n",
    "print(\"ğŸ¯ Dual indexing collections ready for separate image and text embeddings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸  Multimodal processing functions ready!\n",
      "ğŸ“ Ready to index documents with both visual and textual understanding!\n"
     ]
    }
   ],
   "source": [
    "def create_document_id(file_path):\n",
    "    \"\"\"Create a unique document ID based on file path.\"\"\"\n",
    "    return hashlib.md5(file_path.encode()).hexdigest()\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"\n",
    "    Extract text from an image using OCR (pytesseract).\n",
    "    Based on the approach from 02_document_indexing.ipynb\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open and process the image\n",
    "        with Image.open(image_path) as img:\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Extract text using pytesseract\n",
    "            text = pytesseract.image_to_string(img, lang='eng')\n",
    "            \n",
    "            # Clean up the text\n",
    "            text = text.strip()\n",
    "            # Remove excessive whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error extracting text from {os.path.basename(image_path)}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_image_as_array(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and convert it to a numpy array format expected by ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Image as numpy array, or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Convert PIL Image to numpy array\n",
    "            image_array = np.array(img)\n",
    "            return image_array\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error loading image {os.path.basename(image_path)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_image_and_text(image_path):\n",
    "    \"\"\"\n",
    "    Process an image to extract both the image array and OCR text.\n",
    "    This combines visual and textual content for multimodal embedding.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_valid: bool, image_array: numpy.ndarray or None, extracted_text: str)\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image_array = load_image_as_array(image_path)\n",
    "    if image_array is None:\n",
    "        return False, None, \"\"\n",
    "    \n",
    "    # Extract text using OCR\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "    \n",
    "    return True, image_array, extracted_text\n",
    "\n",
    "def process_dual_indexing_batch(image_files, batch_size=25, max_workers=None):\n",
    "    \"\"\"\n",
    "    Process images in batches with DUAL indexing approach.\n",
    "    Creates separate embeddings for images (OpenCLIP) and text (SentenceTransformer).\n",
    "    \n",
    "    Args:\n",
    "        image_files (list): List of (file_path, doc_type) tuples\n",
    "        batch_size (int): Number of images to process at once\n",
    "        max_workers (int): Number of parallel processing workers (None = auto-detect)\n",
    "    \"\"\"\n",
    "    # Auto-detect optimal worker count\n",
    "    if max_workers is None:\n",
    "        max_workers = min(cpu_count(), 6)  # Conservative for stability\n",
    "    \n",
    "    total_files = len(image_files)\n",
    "    processed_count = 0\n",
    "    successful_image_count = 0\n",
    "    successful_text_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"ğŸš€ Starting DUAL indexing of {total_files} documents\")\n",
    "    print(f\"ğŸ“Š Configuration: Batch size: {batch_size}, Processing workers: {max_workers}\")\n",
    "    print(f\"ğŸ¨ Using OpenCLIP for IMAGE embeddings + SentenceTransformer for TEXT embeddings\")\n",
    "    print(f\"ğŸ“ Creating separate collections for visual and textual understanding!\")\n",
    "    \n",
    "    for i in tqdm(range(0, total_files, batch_size), desc=\"Processing dual indexing batches\"):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        \n",
    "        # Step 1: Parallel image and OCR processing\n",
    "        print(f\"  ğŸ” Processing batch {i//batch_size + 1}/{(total_files + batch_size - 1)//batch_size} (image + OCR)\")\n",
    "        \n",
    "        valid_data = []\n",
    "        \n",
    "        # Use parallel processing for image loading and OCR extraction\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit processing tasks (image + OCR)\n",
    "            future_to_file = {\n",
    "                executor.submit(process_image_and_text, file_path): (file_path, doc_type)\n",
    "                for file_path, doc_type in batch_files\n",
    "            }\n",
    "            \n",
    "            # Collect valid data\n",
    "            for future in concurrent.futures.as_completed(future_to_file):\n",
    "                file_path, doc_type = future_to_file[future]\n",
    "                processed_count += 1\n",
    "                \n",
    "                try:\n",
    "                    is_valid, image_array, extracted_text = future.result(timeout=60)  # 60 second timeout for OCR\n",
    "                    \n",
    "                    if is_valid and image_array is not None:\n",
    "                        valid_data.append((file_path, doc_type, image_array, extracted_text))\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        \n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    print(f\"    â° Timeout processing {os.path.basename(file_path)}\")\n",
    "                    failed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"    âŒ Error processing {os.path.basename(file_path)}: {str(e)}\")\n",
    "                    failed_count += 1\n",
    "        \n",
    "        # Step 2: Add to BOTH collections separately\n",
    "        if valid_data:\n",
    "            try:\n",
    "                print(f\"  ğŸ¨ Adding {len(valid_data)} documents to DUAL collections\")\n",
    "                \n",
    "                # Prepare data for IMAGE collection\n",
    "                batch_images = []\n",
    "                batch_image_metadatas = []\n",
    "                batch_image_ids = []\n",
    "                \n",
    "                # Prepare data for TEXT collection\n",
    "                batch_texts = []\n",
    "                batch_text_metadatas = []\n",
    "                batch_text_ids = []\n",
    "                \n",
    "                for file_path, doc_type, image_array, extracted_text in valid_data:\n",
    "                    doc_id = create_document_id(file_path)\n",
    "                    \n",
    "                    # Prepare IMAGE data\n",
    "                    batch_images.append(image_array)\n",
    "                    image_metadata = {\n",
    "                        \"document_type\": doc_type,  # TRUE document type as metadata\n",
    "                        \"file_path\": file_path,\n",
    "                        \"filename\": os.path.basename(file_path),\n",
    "                        \"file_extension\": Path(file_path).suffix.lower(),\n",
    "                        \"indexed_at\": datetime.now().isoformat(),\n",
    "                        \"modality\": \"image\",\n",
    "                        \"has_text\": bool(extracted_text)\n",
    "                    }\n",
    "                    batch_image_metadatas.append(image_metadata)\n",
    "                    batch_image_ids.append(f\"img_{doc_id}\")\n",
    "                    \n",
    "                    # Prepare TEXT data (only if we have extracted text)\n",
    "                    if extracted_text:\n",
    "                        batch_texts.append(extracted_text)\n",
    "                        text_metadata = {\n",
    "                            \"document_type\": doc_type,  # TRUE document type as metadata\n",
    "                            \"file_path\": file_path,\n",
    "                            \"filename\": os.path.basename(file_path),\n",
    "                            \"file_extension\": Path(file_path).suffix.lower(),\n",
    "                            \"indexed_at\": datetime.now().isoformat(),\n",
    "                            \"modality\": \"text\",\n",
    "                            \"text_length\": len(extracted_text),\n",
    "                            \"extracted_text\": extracted_text\n",
    "                        }\n",
    "                        batch_text_metadatas.append(text_metadata)\n",
    "                        batch_text_ids.append(f\"txt_{doc_id}\")\n",
    "                \n",
    "                # Add to IMAGE collection\n",
    "                image_collection.add(\n",
    "                    images=batch_images,\n",
    "                    metadatas=batch_image_metadatas,\n",
    "                    ids=batch_image_ids\n",
    "                )\n",
    "                successful_image_count += len(batch_images)\n",
    "                \n",
    "                # Add to TEXT collection (only if we have text data)\n",
    "                if batch_texts:\n",
    "                    text_collection.add(\n",
    "                        documents=batch_texts,\n",
    "                        metadatas=batch_text_metadatas,\n",
    "                        ids=batch_text_ids\n",
    "                    )\n",
    "                    successful_text_count += len(batch_texts)\n",
    "                \n",
    "                print(f\"  âœ… Batch {i//batch_size + 1} completed:\")\n",
    "                print(f\"     ğŸ“¸ {len(batch_images)} image embeddings added\")\n",
    "                print(f\"     ğŸ“ {len(batch_texts)} text embeddings added\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error processing batch {i//batch_size + 1}: {str(e)}\")\n",
    "                failed_count += len(valid_data)\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  âš ï¸  No valid documents in batch {i//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DUAL indexing complete!\")\n",
    "    print(f\"ğŸ“ˆ Total files processed: {processed_count}\")\n",
    "    print(f\"âœ… Image embeddings created: {successful_image_count}\")\n",
    "    print(f\"âœ… Text embeddings created: {successful_text_count}\")\n",
    "    print(f\"âŒ Failed: {failed_count}\")\n",
    "    print(f\"ğŸ“š IMAGE collection now contains: {image_collection.count()} embeddings\")\n",
    "    print(f\"ğŸ“š TEXT collection now contains: {text_collection.count()} embeddings\")\n",
    "    print(f\"ğŸ¯ Dual indexing enables confidence-based classification!\")\n",
    "\n",
    "print(\"ğŸ› ï¸  Multimodal processing functions ready!\")\n",
    "print(\"ğŸ“ Ready to index documents with both visual and textual understanding!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Recommended settings for multimodal processing:\n",
      "   CPU cores: 14\n",
      "   Memory: 48GB\n",
      "   Recommended workers: 4\n",
      "   Recommended batch size: 15\n",
      "   ğŸ“ Optimized for: Image processing + OCR extraction\n",
      "ğŸ’» System Status:\n",
      "   CPU Usage: 37.8%\n",
      "   Memory Usage: 57.2% (24.0GB / 48.0GB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37.8, 57.2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# System optimization settings\n",
    "def get_optimal_settings():\n",
    "    \"\"\"Get optimal processing settings based on system resources for multimodal processing.\"\"\"\n",
    "    cpu_count_total = cpu_count()\n",
    "    memory_gb = psutil.virtual_memory().total // (1024**3)\n",
    "    \n",
    "    # Conservative settings for multimodal processing (image + OCR)\n",
    "    # OCR requires more CPU, so slightly smaller batches and workers\n",
    "    if memory_gb >= 16:\n",
    "        recommended_workers = min(4, cpu_count_total - 2)  # Reduced for OCR stability\n",
    "        recommended_batch_size = 15  # Smaller batches for OCR + image arrays\n",
    "    elif memory_gb >= 8:\n",
    "        recommended_workers = min(3, cpu_count_total - 1)\n",
    "        recommended_batch_size = 10\n",
    "    else:\n",
    "        recommended_workers = min(2, cpu_count_total - 1)\n",
    "        recommended_batch_size = 8\n",
    "    \n",
    "    print(f\"ğŸ¯ Recommended settings for multimodal processing:\")\n",
    "    print(f\"   CPU cores: {cpu_count_total}\")\n",
    "    print(f\"   Memory: {memory_gb}GB\")\n",
    "    print(f\"   Recommended workers: {recommended_workers}\")\n",
    "    print(f\"   Recommended batch size: {recommended_batch_size}\")\n",
    "    print(f\"   ğŸ“ Optimized for: Image processing + OCR extraction\")\n",
    "    \n",
    "    return recommended_workers, recommended_batch_size\n",
    "\n",
    "def monitor_system_resources():\n",
    "    \"\"\"Monitor CPU and memory usage.\"\"\"\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    memory = psutil.virtual_memory()\n",
    "    \n",
    "    print(f\"ğŸ’» System Status:\")\n",
    "    print(f\"   CPU Usage: {cpu_percent:.1f}%\")\n",
    "    print(f\"   Memory Usage: {memory.percent:.1f}% ({memory.used // (1024**3):.1f}GB / {memory.total // (1024**3):.1f}GB)\")\n",
    "    \n",
    "    if cpu_percent > 80:\n",
    "        print(\"   âš ï¸  High CPU usage - consider reducing max_workers\")\n",
    "    if memory.percent > 85:\n",
    "        print(\"   âš ï¸  High memory usage - consider reducing batch_size\")\n",
    "    \n",
    "    return cpu_percent, memory.percent\n",
    "\n",
    "# Get system recommendations\n",
    "recommended_workers, recommended_batch_size = get_optimal_settings()\n",
    "# For testing purposes\n",
    "recommended_workers = 16\n",
    "recommended_batch_size = 16 * 10\n",
    "monitor_system_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Processing all documents with multimodal embeddings...\n",
      "ğŸ“ Combining image analysis + OCR text extraction for each document\n",
      "ğŸ’» System Status:\n",
      "   CPU Usage: 39.5%\n",
      "   Memory Usage: 57.2% (23.0GB / 48.0GB)\n",
      "ğŸš€ Starting DUAL indexing of 3494 documents\n",
      "ğŸ“Š Configuration: Batch size: 160, Processing workers: 16\n",
      "ğŸ¨ Using OpenCLIP for IMAGE embeddings + SentenceTransformer for TEXT embeddings\n",
      "ğŸ“ Creating separate collections for visual and textual understanding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ” Processing batch 1/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:   5%|â–         | 1/22 [00:11<04:06, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 1 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 160 text embeddings added\n",
      "  ğŸ” Processing batch 2/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:   9%|â–‰         | 2/22 [00:26<04:28, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 2 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 157 text embeddings added\n",
      "  ğŸ” Processing batch 3/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  14%|â–ˆâ–        | 3/22 [00:40<04:17, 13.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 3 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 154 text embeddings added\n",
      "  ğŸ” Processing batch 4/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  18%|â–ˆâ–Š        | 4/22 [00:51<03:48, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 4 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 142 text embeddings added\n",
      "  ğŸ” Processing batch 5/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  23%|â–ˆâ–ˆâ–       | 5/22 [01:08<04:04, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 5 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 159 text embeddings added\n",
      "  ğŸ” Processing batch 6/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  27%|â–ˆâ–ˆâ–‹       | 6/22 [01:23<03:53, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 6 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 159 text embeddings added\n",
      "  ğŸ” Processing batch 7/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  32%|â–ˆâ–ˆâ–ˆâ–      | 7/22 [01:36<03:31, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 7 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 158 text embeddings added\n",
      "  ğŸ” Processing batch 8/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 8/22 [01:51<03:21, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 8 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 160 text embeddings added\n",
      "  ğŸ” Processing batch 9/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 9/22 [02:06<03:07, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 9 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 160 text embeddings added\n",
      "  ğŸ” Processing batch 10/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10/22 [02:20<02:51, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 10 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 158 text embeddings added\n",
      "  ğŸ” Processing batch 11/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 11/22 [02:34<02:38, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 11 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 157 text embeddings added\n",
      "  ğŸ” Processing batch 12/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 12/22 [02:49<02:23, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 12 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 158 text embeddings added\n",
      "  ğŸ” Processing batch 13/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 13/22 [03:12<02:34, 17.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 13 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 158 text embeddings added\n",
      "  ğŸ” Processing batch 14/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 14/22 [03:28<02:14, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 14 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 141 text embeddings added\n",
      "  ğŸ” Processing batch 15/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 15/22 [03:39<01:44, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 15 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 121 text embeddings added\n",
      "  ğŸ” Processing batch 16/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 16/22 [03:51<01:23, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 16 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 141 text embeddings added\n",
      "  ğŸ” Processing batch 17/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 17/22 [04:04<01:09, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 17 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 155 text embeddings added\n",
      "  ğŸ” Processing batch 18/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 18/22 [04:17<00:54, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 18 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 160 text embeddings added\n",
      "  ğŸ” Processing batch 19/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 19/22 [04:30<00:40, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 19 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 160 text embeddings added\n",
      "  ğŸ” Processing batch 20/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 20/22 [04:44<00:26, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 20 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 159 text embeddings added\n",
      "  ğŸ” Processing batch 21/22 (image + OCR)\n",
      "  ğŸ¨ Adding 160 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 21/22 [04:58<00:13, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 21 completed:\n",
      "     ğŸ“¸ 160 image embeddings added\n",
      "     ğŸ“ 155 text embeddings added\n",
      "  ğŸ” Processing batch 22/22 (image + OCR)\n",
      "  ğŸ¨ Adding 134 documents to DUAL collections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dual indexing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [05:09<00:00, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… Batch 22 completed:\n",
      "     ğŸ“¸ 134 image embeddings added\n",
      "     ğŸ“ 134 text embeddings added\n",
      "\n",
      "ğŸ‰ DUAL indexing complete!\n",
      "ğŸ“ˆ Total files processed: 3494\n",
      "âœ… Image embeddings created: 3494\n",
      "âœ… Text embeddings created: 3366\n",
      "âŒ Failed: 0\n",
      "ğŸ“š IMAGE collection now contains: 3494 embeddings\n",
      "ğŸ“š TEXT collection now contains: 3366 embeddings\n",
      "ğŸ¯ Dual indexing enables confidence-based classification!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» System Status:\n",
      "   CPU Usage: 38.2%\n",
      "   Memory Usage: 57.1% (22.0GB / 48.0GB)\n",
      "â±ï¸  Total processing time: 310.86 seconds\n",
      "ğŸ“Š Average processing speed: 11.24 documents/second\n",
      "ğŸ¯ Each document now has multimodal embeddings for better search accuracy!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nCUSTOM_BATCH_SIZE = 10\\nCUSTOM_MAX_WORKERS = 2\\nCUSTOM_SUBSET = image_files[:100]  # Process specific range\\n\\nprint(f\"ğŸ› ï¸  Running custom multimodal processing...\")\\nprint(f\"   Batch size: {CUSTOM_BATCH_SIZE}\")\\nprint(f\"   Workers: {CUSTOM_MAX_WORKERS}\")\\nprint(f\"   Processing {len(CUSTOM_SUBSET)} documents\")\\n\\nprocess_multimodal_batch(\\n    CUSTOM_SUBSET,\\n    batch_size=CUSTOM_BATCH_SIZE,\\n    max_workers=CUSTOM_MAX_WORKERS\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Process documents with multimodal embeddings (image + OCR text)\n",
    "print(\"ğŸš€ Choose your multimodal processing approach:\\n\")\n",
    "\n",
    "print(\"Option A: Test with small subset (recommended for first run)\")\n",
    "print(\"- Process first 50 documents to test multimodal system\")\n",
    "print(\"- Combines visual analysis + OCR text extraction\")\n",
    "print(\"- Uses conservative settings to ensure stability\")\n",
    "print()\n",
    "\n",
    "print(\"Option B: Process all documents with optimal settings\")\n",
    "print(\"- Uses system-optimized settings for full dataset\")\n",
    "print(\"- Full multimodal indexing (image + OCR text)\")\n",
    "print(\"- Recommended after testing with Option A\")\n",
    "print()\n",
    "\n",
    "print(\"Option C: Custom processing\")\n",
    "print(\"- Customize batch size and worker count for your needs\")\n",
    "print(\"- Full control over multimodal processing parameters\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ’¡ About Multimodal Indexing:\")\n",
    "print(\"- Each document gets embedded using BOTH visual content AND extracted text\")\n",
    "print(\"- OpenCLIP understands relationships between images and text\")\n",
    "print(\"- This significantly improves search accuracy and relevance\")\n",
    "print()\n",
    "\n",
    "# Uncomment ONE of the options below:\n",
    "\n",
    "# Option A: Test run (recommended first)\n",
    "print(\"ğŸ”¬ Running test with first 50 documents (multimodal)...\")\n",
    "process_multimodal_batch(\n",
    "    image_files[:50],  # Test with first 50 documents\n",
    "    batch_size=8,  # Smaller batches for OCR processing\n",
    "    max_workers=recommended_workers // 2  # Use half recommended workers for safety\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Option B: Full processing with optimal settings (uncomment to use)\n",
    "\n",
    "print(\"ğŸš€ Processing all documents with multimodal embeddings...\")\n",
    "print(\"ğŸ“ Combining image analysis + OCR text extraction for each document\")\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "initial_cpu, initial_memory = monitor_system_resources()\n",
    "\n",
    "process_dual_indexing_batch(\n",
    "    image_files,\n",
    "    batch_size=recommended_batch_size,\n",
    "    max_workers=recommended_workers\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "final_cpu, final_memory = monitor_system_resources()\n",
    "\n",
    "print(f\"â±ï¸  Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"ğŸ“Š Average processing speed: {len(image_files) / (end_time - start_time):.2f} documents/second\")\n",
    "print(f\"ğŸ¯ Each document now has multimodal embeddings for better search accuracy!\")\n",
    "\n",
    "\n",
    "# Option C: Custom processing (uncomment and customize)\n",
    "\"\"\"\n",
    "CUSTOM_BATCH_SIZE = 10\n",
    "CUSTOM_MAX_WORKERS = 2\n",
    "CUSTOM_SUBSET = image_files[:100]  # Process specific range\n",
    "\n",
    "print(f\"ğŸ› ï¸  Running custom multimodal processing...\")\n",
    "print(f\"   Batch size: {CUSTOM_BATCH_SIZE}\")\n",
    "print(f\"   Workers: {CUSTOM_MAX_WORKERS}\")\n",
    "print(f\"   Processing {len(CUSTOM_SUBSET)} documents\")\n",
    "\n",
    "process_multimodal_batch(\n",
    "    CUSTOM_SUBSET,\n",
    "    batch_size=CUSTOM_BATCH_SIZE,\n",
    "    max_workers=CUSTOM_MAX_WORKERS\n",
    ")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUAL Indexing Results ===\n",
      "ğŸ“¸ IMAGE collection: 3494 documents\n",
      "ğŸ“ TEXT collection: 3366 documents\n",
      "\n",
      "ğŸ“Š Sample from IMAGE collection:\n",
      "  1. 92600841_0845.jpg (type: form)\n",
      "  2. 2050755264.jpg (type: form)\n",
      "  3. 508881563+-1563.jpg (type: form)\n",
      "\n",
      "ğŸ“Š Sample from TEXT collection:\n",
      "  1. 92600841_0845.jpg - \"DATE: LORILLARD NAME/LIST PULL REQUEST To: S. R. Benson xc: C. Humphrey A. Pasheluk V. Lindsley E, D...\"\n",
      "  2. 2050755264.jpg - \"Ip, Vee. {Â¥yo Lorsc â€”â€” Feinmwhay OBR VIVES PHILIP MORRIS CORPORATE SERVICES INC. Tharp lS waked AS W...\"\n",
      "  3. 508881563+-1563.jpg - \"Beltrage zur Tabakforschung International Copy for anor Reviewer's Comments* Title of manuscript: Th...\"\n",
      "\n",
      "âœ… Dual indexing complete! Ready for confidence-based classification.\n"
     ]
    }
   ],
   "source": [
    "# Verify the DUAL indexing results\n",
    "print(\"=== DUAL Indexing Results ===\")\n",
    "print(f\"ğŸ“¸ IMAGE collection: {image_collection.count()} documents\")\n",
    "print(f\"ğŸ“ TEXT collection: {text_collection.count()} documents\")\n",
    "\n",
    "# Get some statistics from both collections\n",
    "image_results = image_collection.get(limit=5, include=[\"metadatas\"])\n",
    "text_results = text_collection.get(limit=5, include=[\"metadatas\"])\n",
    "\n",
    "if image_results['metadatas']:\n",
    "    print(f\"\\nğŸ“Š Sample from IMAGE collection:\")\n",
    "    for i, metadata in enumerate(image_results['metadatas'][:3]):\n",
    "        print(f\"  {i+1}. {metadata['filename']} (type: {metadata['document_type']})\")\n",
    "\n",
    "if text_results['metadatas']:\n",
    "    print(f\"\\nğŸ“Š Sample from TEXT collection:\")  \n",
    "    for i, metadata in enumerate(text_results['metadatas'][:3]):\n",
    "        text_preview = metadata.get('extracted_text', '')[:100]\n",
    "        print(f\"  {i+1}. {metadata['filename']} - \\\"{text_preview}...\\\"\")\n",
    "\n",
    "print(f\"\\nâœ… Dual indexing complete! Ready for confidence-based classification.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing DUAL Confidence Classification ===\n",
      "ğŸ¯ Testing confidence-based classification using separate image and text collections!\n",
      "\n",
      "ğŸ§ª Testing 5 sample documents with dual confidence classification:\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª TEST 1/3\n",
      "================================================================================\n",
      "ğŸ” DUAL confidence search for: ti31689101.jpg\n",
      "ğŸ“ Extracted OCR text: Yes (405 chars)\n",
      "ğŸ“¸ Searching IMAGE collection...\n",
      "  IMAGE classification: invoice (confidence: 61.1%)\n",
      "ğŸ“ Searching TEXT collection...\n",
      "  TEXT classification: invoice (confidence: 47.3%)\n",
      "\n",
      "ğŸ¯ CONFIDENCE COMPARISON:\n",
      "   ğŸ“¸ Image: invoice (61.1%)\n",
      "   ğŸ“ Text:  invoice (47.3%)\n",
      "   ğŸ† Winner: IMAGE - invoice (61.1%)\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ DUAL CONFIDENCE CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "ğŸ“ Query Image: ti31689101.jpg\n",
      "ğŸ“ OCR Text Available: Yes\n",
      "   First 100 chars: INVOICE Fla FANNON.LUERS ASSOCIATES INC. 5352 a6th Ave, Hyatievile, Ma 20761 + G01) SONaTTE TOBACCO ...\n",
      "\n",
      "ğŸ“‹ TRUE DOCUMENT TYPE: invoice\n",
      "ğŸ¯ PREDICTION ACCURACY: âœ… CORRECT\n",
      "\n",
      "ğŸ” INDIVIDUAL MODALITY RESULTS:\n",
      "   ğŸ“¸ Image Classification: invoice (61.1%) âœ…\n",
      "   ğŸ“ Text Classification:  invoice (47.3%) âœ…\n",
      "\n",
      "ğŸ† FINAL CLASSIFICATION:\n",
      "   ğŸ“‹ Predicted Type: invoice\n",
      "   ğŸ¯ Confidence: 61.1%\n",
      "   ğŸ… Winning Modality: image\n",
      "   ğŸ¯ Overall Accuracy: âœ… CORRECT\n",
      "\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª TEST 2/3\n",
      "================================================================================\n",
      "ğŸ” DUAL confidence search for: 92600841_0845.jpg\n",
      "ğŸ“ Extracted OCR text: Yes (126 chars)\n",
      "ğŸ“¸ Searching IMAGE collection...\n",
      "  IMAGE classification: form (confidence: 59.9%)\n",
      "ğŸ“ Searching TEXT collection...\n",
      "  TEXT classification: form (confidence: 73.4%)\n",
      "\n",
      "ğŸ¯ CONFIDENCE COMPARISON:\n",
      "   ğŸ“¸ Image: form (59.9%)\n",
      "   ğŸ“ Text:  form (73.4%)\n",
      "   ğŸ† Winner: TEXT - form (73.4%)\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ DUAL CONFIDENCE CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "ğŸ“ Query Image: 92600841_0845.jpg\n",
      "ğŸ“ OCR Text Available: Yes\n",
      "   First 100 chars: DATE: LORILLARD NAME/LIST PULL REQUEST To: S. R. Benson xc: C. Humphrey A. Pasheluk V. Lindsley E, D...\n",
      "\n",
      "ğŸ“‹ TRUE DOCUMENT TYPE: form\n",
      "ğŸ¯ PREDICTION ACCURACY: âœ… CORRECT\n",
      "\n",
      "ğŸ” INDIVIDUAL MODALITY RESULTS:\n",
      "   ğŸ“¸ Image Classification: form (59.9%) âœ…\n",
      "   ğŸ“ Text Classification:  form (73.4%) âœ…\n",
      "\n",
      "ğŸ† FINAL CLASSIFICATION:\n",
      "   ğŸ“‹ Predicted Type: form\n",
      "   ğŸ¯ Confidence: 73.4%\n",
      "   ğŸ… Winning Modality: text\n",
      "   ğŸ¯ Overall Accuracy: âœ… CORRECT\n",
      "\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª TEST 3/3\n",
      "================================================================================\n",
      "ğŸ” DUAL confidence search for: 509206571.jpg\n",
      "ğŸ“ Extracted OCR text: Yes (216 chars)\n",
      "ğŸ“¸ Searching IMAGE collection...\n",
      "  IMAGE classification: file_folder (confidence: 58.0%)\n",
      "ğŸ“ Searching TEXT collection...\n",
      "  TEXT classification: handwritten (confidence: 100.0%)\n",
      "\n",
      "ğŸ¯ CONFIDENCE COMPARISON:\n",
      "   ğŸ“¸ Image: file_folder (58.0%)\n",
      "   ğŸ“ Text:  handwritten (100.0%)\n",
      "   ğŸ† Winner: TEXT - handwritten (100.0%)\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ DUAL CONFIDENCE CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "ğŸ“ Query Image: 509206571.jpg\n",
      "ğŸ“ OCR Text Available: Yes\n",
      "   First 100 chars: MEMO From the Desk of February 10, 1977 Dr. Frank G. Colby if you want this we can send you a copy, ...\n",
      "\n",
      "ğŸ“‹ TRUE DOCUMENT TYPE: handwritten\n",
      "ğŸ¯ PREDICTION ACCURACY: âœ… CORRECT\n",
      "\n",
      "ğŸ” INDIVIDUAL MODALITY RESULTS:\n",
      "   ğŸ“¸ Image Classification: file_folder (58.0%) âŒ\n",
      "   ğŸ“ Text Classification:  handwritten (100.0%) âœ…\n",
      "\n",
      "ğŸ† FINAL CLASSIFICATION:\n",
      "   ğŸ“‹ Predicted Type: handwritten\n",
      "   ğŸ¯ Confidence: 100.0%\n",
      "   ğŸ… Winning Modality: text\n",
      "   ğŸ¯ Overall Accuracy: âœ… CORRECT\n",
      "\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "âœ… DUAL confidence classification system is ready!\n",
      "\n",
      "ğŸ“‹ Available methods:\n",
      "  1. search_with_dual_confidence(query_image_path) - Main classification function\n",
      "  2. Automatically searches both IMAGE and TEXT collections\n",
      "  3. Returns confidence-based final classification\n",
      "  4. Shows which modality (image or text) won the classification\n",
      "\n",
      "ğŸ“Š Current collection status:\n",
      "   ğŸ“¸ IMAGE collection: 3494 embeddings\n",
      "   ğŸ“ TEXT collection: 3366 embeddings\n",
      "ğŸ¯ Ready for confidence-based document classification!\n"
     ]
    }
   ],
   "source": [
    "# DUAL search and confidence-based classification functionality\n",
    "def search_with_dual_confidence(query_image_path, n_results=10):\n",
    "    \"\"\"\n",
    "    Search using DUAL indexing approach with confidence-based classification.\n",
    "    \n",
    "    This function:\n",
    "    1. Searches the image collection using the query image\n",
    "    2. Extracts OCR text from the query image and searches the text collection\n",
    "    3. Analyzes classification confidence from both modalities\n",
    "    4. Returns final classification based on higher confidence\n",
    "    \n",
    "    Args:\n",
    "        query_image_path (str): Path to query image\n",
    "        n_results (int): Number of results to return from each collection\n",
    "        \n",
    "    Returns:\n",
    "        dict: Combined results with confidence-based classification\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” DUAL confidence search for: {os.path.basename(query_image_path)}\")\n",
    "    \n",
    "    # Step 1: Load and process query image\n",
    "    query_image_array = load_image_as_array(query_image_path)\n",
    "    if query_image_array is None:\n",
    "        print(f\"âŒ Failed to load query image: {query_image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Extract OCR text from query image\n",
    "    query_text = extract_text_from_image(query_image_path)\n",
    "    print(f\"ğŸ“ Extracted OCR text: {'Yes' if query_text else 'No'} ({len(query_text)} chars)\")\n",
    "    \n",
    "    # Step 3: Search IMAGE collection\n",
    "    try:\n",
    "        print(f\"ğŸ“¸ Searching IMAGE collection...\")\n",
    "        image_results = image_collection.query(\n",
    "            query_images=[query_image_array],\n",
    "            n_results=n_results,\n",
    "            include=[\"metadatas\", \"distances\"]\n",
    "        )\n",
    "        image_classification, image_confidence = analyze_classification_confidence(\n",
    "            image_results, \"image\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error searching image collection: {str(e)}\")\n",
    "        image_classification, image_confidence = None, 0.0\n",
    "    \n",
    "    # Step 4: Search TEXT collection (if we have text)\n",
    "    text_classification, text_confidence = None, 0.0\n",
    "    if query_text:\n",
    "        try:\n",
    "            print(f\"ğŸ“ Searching TEXT collection...\")\n",
    "            text_results = text_collection.query(\n",
    "                query_texts=[query_text],\n",
    "                n_results=n_results,\n",
    "                include=[\"metadatas\", \"distances\"]\n",
    "            )\n",
    "            text_classification, text_confidence = analyze_classification_confidence(\n",
    "                text_results, \"text\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error searching text collection: {str(e)}\")\n",
    "    \n",
    "    # Step 5: Confidence-based final classification\n",
    "    final_classification, final_confidence, winning_modality = determine_final_classification(\n",
    "        image_classification, image_confidence,\n",
    "        text_classification, text_confidence\n",
    "    )\n",
    "    \n",
    "    # Step 6: Create combined results\n",
    "    results = {\n",
    "        \"query_image\": query_image_path,\n",
    "        \"query_text\": query_text,\n",
    "        \"image_classification\": image_classification,\n",
    "        \"image_confidence\": image_confidence,\n",
    "        \"text_classification\": text_classification, \n",
    "        \"text_confidence\": text_confidence,\n",
    "        \"final_classification\": final_classification,\n",
    "        \"final_confidence\": final_confidence,\n",
    "        \"winning_modality\": winning_modality,\n",
    "        \"image_results\": image_results if 'image_results' in locals() else None,\n",
    "        \"text_results\": text_results if 'text_results' in locals() else None\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_classification_confidence(search_results, modality):\n",
    "    \"\"\"\n",
    "    Analyze classification confidence based on search results.\n",
    "    Uses distance-based voting with confidence scoring.\n",
    "    \n",
    "    Args:\n",
    "        search_results: ChromaDB search results\n",
    "        modality (str): \"image\" or \"text\"\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (predicted_class, confidence_score)\n",
    "    \"\"\"\n",
    "    if not search_results or not search_results['metadatas'][0]:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Count document types weighted by similarity\n",
    "    doc_type_scores = {}\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metadata, distance in zip(search_results['metadatas'][0], search_results['distances'][0]):\n",
    "        doc_type = metadata['document_type']\n",
    "        # Convert distance to similarity (closer = higher similarity)\n",
    "        similarity = 1 - distance\n",
    "        # Weight by similarity (higher similarity = more influence)\n",
    "        weight = max(0, similarity)  # Ensure non-negative\n",
    "        \n",
    "        if doc_type not in doc_type_scores:\n",
    "            doc_type_scores[doc_type] = 0\n",
    "        doc_type_scores[doc_type] += weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Normalize scores to get confidence percentages\n",
    "    for doc_type in doc_type_scores:\n",
    "        doc_type_scores[doc_type] /= total_weight\n",
    "    \n",
    "    # Get the top prediction and its confidence\n",
    "    predicted_class = max(doc_type_scores, key=doc_type_scores.get)\n",
    "    confidence = doc_type_scores[predicted_class]\n",
    "    \n",
    "    print(f\"  {modality.upper()} classification: {predicted_class} (confidence: {confidence:.1%})\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "def determine_final_classification(image_class, image_conf, text_class, text_conf):\n",
    "    \"\"\"\n",
    "    Determine final classification based on confidence levels from both modalities.\n",
    "    \n",
    "    Args:\n",
    "        image_class: Image-based classification\n",
    "        image_conf: Image-based confidence\n",
    "        text_class: Text-based classification  \n",
    "        text_conf: Text-based confidence\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (final_class, final_confidence, winning_modality)\n",
    "    \"\"\"\n",
    "    # If only one modality has results\n",
    "    if image_class and not text_class:\n",
    "        return image_class, image_conf, \"image_only\"\n",
    "    elif text_class and not image_class:\n",
    "        return text_class, text_conf, \"text_only\"\n",
    "    elif not image_class and not text_class:\n",
    "        return None, 0.0, \"no_results\"\n",
    "    \n",
    "    # Both modalities have results - compare confidence\n",
    "    print(f\"\\nğŸ¯ CONFIDENCE COMPARISON:\")\n",
    "    print(f\"   ğŸ“¸ Image: {image_class} ({image_conf:.1%})\")\n",
    "    print(f\"   ğŸ“ Text:  {text_class} ({text_conf:.1%})\")\n",
    "    \n",
    "    # Choose based on higher confidence\n",
    "    if image_conf > text_conf:\n",
    "        winning_modality = \"image\"\n",
    "        final_class = image_class\n",
    "        final_confidence = image_conf\n",
    "    elif text_conf > image_conf:\n",
    "        winning_modality = \"text\"\n",
    "        final_class = text_class\n",
    "        final_confidence = text_conf\n",
    "    else:\n",
    "        # Tie - prefer image (or could use other tie-breaking logic)\n",
    "        winning_modality = \"image_tie\"\n",
    "        final_class = image_class\n",
    "        final_confidence = image_conf\n",
    "    \n",
    "    print(f\"   ğŸ† Winner: {winning_modality.upper()} - {final_class} ({final_confidence:.1%})\")\n",
    "    \n",
    "    return final_class, final_confidence, winning_modality\n",
    "\n",
    "def get_true_document_type(query_image_path):\n",
    "    \"\"\"\n",
    "    Get the true document type for a query image from the collections.\n",
    "    \n",
    "    Args:\n",
    "        query_image_path (str): Path to the query image\n",
    "        \n",
    "    Returns:\n",
    "        str: True document type, or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to find the document in the image collection first\n",
    "        doc_id = create_document_id(query_image_path)\n",
    "        image_id = f\"img_{doc_id}\"\n",
    "        \n",
    "        # Get from image collection\n",
    "        result = image_collection.get(\n",
    "            ids=[image_id],\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        if result['metadatas'] and len(result['metadatas']) > 0:\n",
    "            return result['metadatas'][0]['document_type']\n",
    "        \n",
    "        # If not found in image collection, try text collection\n",
    "        text_id = f\"txt_{doc_id}\"\n",
    "        result = text_collection.get(\n",
    "            ids=[text_id],\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        if result['metadatas'] and len(result['metadatas']) > 0:\n",
    "            return result['metadatas'][0]['document_type']\n",
    "            \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error getting true document type: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def display_dual_search_results(results):\n",
    "    \"\"\"Display results from dual confidence search in a readable format.\"\"\"\n",
    "    if not results:\n",
    "        print(\"âŒ No results to display\")\n",
    "        return\n",
    "    \n",
    "    # Get the true document type from the collections\n",
    "    true_doc_type = get_true_document_type(results['query_image'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ¯ DUAL CONFIDENCE CLASSIFICATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ğŸ“ Query Image: {os.path.basename(results['query_image'])}\")\n",
    "    print(f\"ğŸ“ OCR Text Available: {'Yes' if results['query_text'] else 'No'}\")\n",
    "    if results['query_text']:\n",
    "        print(f\"   First 100 chars: {results['query_text'][:100]}...\")\n",
    "    \n",
    "    # Display true document type\n",
    "    if true_doc_type:\n",
    "        print(f\"\\nğŸ“‹ TRUE DOCUMENT TYPE: {true_doc_type}\")\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        is_correct = results['final_classification'] == true_doc_type\n",
    "        accuracy_symbol = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"ğŸ¯ PREDICTION ACCURACY: {accuracy_symbol} {'CORRECT' if is_correct else 'INCORRECT'}\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“‹ TRUE DOCUMENT TYPE: Unknown (not found in collections)\")\n",
    "    \n",
    "    print(f\"\\nğŸ” INDIVIDUAL MODALITY RESULTS:\")\n",
    "    image_correct = results['image_classification'] == true_doc_type if true_doc_type else False\n",
    "    text_correct = results['text_classification'] == true_doc_type if true_doc_type else False\n",
    "    \n",
    "    image_symbol = \"âœ…\" if image_correct else \"âŒ\" if true_doc_type else \"â“\"\n",
    "    text_symbol = \"âœ…\" if text_correct else \"âŒ\" if true_doc_type else \"â“\"\n",
    "    \n",
    "    print(f\"   ğŸ“¸ Image Classification: {results['image_classification']} ({results['image_confidence']:.1%}) {image_symbol}\")\n",
    "    print(f\"   ğŸ“ Text Classification:  {results['text_classification']} ({results['text_confidence']:.1%}) {text_symbol}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† FINAL CLASSIFICATION:\")\n",
    "    print(f\"   ğŸ“‹ Predicted Type: {results['final_classification']}\")\n",
    "    print(f\"   ğŸ¯ Confidence: {results['final_confidence']:.1%}\")\n",
    "    print(f\"   ğŸ… Winning Modality: {results['winning_modality']}\")\n",
    "    \n",
    "    if true_doc_type:\n",
    "        is_correct = results['final_classification'] == true_doc_type\n",
    "        print(f\"   ğŸ¯ Overall Accuracy: {'âœ… CORRECT' if is_correct else 'âŒ INCORRECT'}\")\n",
    "        \n",
    "        # Additional insights\n",
    "        if not is_correct:\n",
    "            print(f\"   ğŸ’¡ Note: True type was '{true_doc_type}', predicted '{results['final_classification']}'\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Example searches using DUAL confidence approach\n",
    "if image_collection.count() > 0 and text_collection.count() > 0:\n",
    "    print(\"=== Testing DUAL Confidence Classification ===\")\n",
    "    print(\"ğŸ¯ Testing confidence-based classification using separate image and text collections!\")\n",
    "    \n",
    "    # Get some sample documents from different types for testing\n",
    "    sample_types = [\"invoice\", \"form\", \"handwritten\", \"budget\", \"email\"]\n",
    "    test_samples = []\n",
    "    \n",
    "    for doc_type in sample_types:\n",
    "        # Try to get a sample from this document type\n",
    "        type_results = image_collection.get(\n",
    "            where={\"document_type\": doc_type},\n",
    "            limit=1,\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        if type_results['metadatas']:\n",
    "            test_samples.append(type_results['metadatas'][0]['file_path'])\n",
    "    \n",
    "    # If we don't have enough samples, just get some random ones\n",
    "    if len(test_samples) < 3:\n",
    "        random_results = image_collection.get(limit=5, include=[\"metadatas\"])\n",
    "        for metadata in random_results['metadatas']:\n",
    "            if metadata['file_path'] not in test_samples:\n",
    "                test_samples.append(metadata['file_path'])\n",
    "                if len(test_samples) >= 5:\n",
    "                    break\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Testing {len(test_samples)} sample documents with dual confidence classification:\")\n",
    "    \n",
    "    for i, sample_path in enumerate(test_samples[:3]):  # Test first 3 samples\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸ§ª TEST {i+1}/{min(len(test_samples), 3)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Run dual confidence search\n",
    "        results = search_with_dual_confidence(sample_path, n_results=10)\n",
    "        if results:\n",
    "            display_dual_search_results(results)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… DUAL confidence classification system is ready!\")\n",
    "    print(\"\\nğŸ“‹ Available methods:\")\n",
    "    print(\"  1. search_with_dual_confidence(query_image_path) - Main classification function\")\n",
    "    print(\"  2. Automatically searches both IMAGE and TEXT collections\")\n",
    "    print(\"  3. Returns confidence-based final classification\")\n",
    "    print(\"  4. Shows which modality (image or text) won the classification\")\n",
    "    print(f\"\\nğŸ“Š Current collection status:\")\n",
    "    print(f\"   ğŸ“¸ IMAGE collection: {image_collection.count()} embeddings\")\n",
    "    print(f\"   ğŸ“ TEXT collection: {text_collection.count()} embeddings\")\n",
    "    print(f\"ğŸ¯ Ready for confidence-based document classification!\")\n",
    "    \n",
    "elif image_collection.count() > 0:\n",
    "    print(\"âš ï¸  Only IMAGE collection has data. Run the dual indexing processing to populate TEXT collection.\")\n",
    "    print(f\"ğŸ“¸ IMAGE collection: {image_collection.count()} embeddings\")\n",
    "    print(f\"ğŸ“ TEXT collection: {text_collection.count()} embeddings\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents indexed yet. Run the processing cell first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
